{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm import tqdm, tqdm_notebook, _tqdm_notebook, tqdm_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_fmt = \"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\"\n",
    "logging.basicConfig(format=log_fmt, level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_day(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[0][1:]))\n",
    "\n",
    "def extract_hour(s):\n",
    "    return s.apply(lambda x: int(x.split('-')[1][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_str(d):\n",
    "    return np.array(list(map(float, d.split())))\n",
    "def parse_list_1(d):\n",
    "    if d == '-1':\n",
    "        return [0]\n",
    "    return list(map(lambda x: int(x[1:]), str(d).split(',')))\n",
    "def parse_map(d):\n",
    "    if d == '-1':\n",
    "        return {}\n",
    "    return dict([int(z.split(':')[0][1:]), float(z.split(':')[1])] for z in d.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-12-17 00:49:08,653] INFO in <ipython-input-11-01d39a31e3c0>: invite (9489162, 4)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 4 elements, new values have 3 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-01d39a31e3c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# test = pd.read_csv(f'{base_path}/invite_info_evaluate_1_0926.txt', sep='\\t', header=None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{base_path}/sample_sub_1.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'qid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'uid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   5190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5191\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5192\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5193\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5194\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/properties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    181\u001b[0m             raise ValueError(\n\u001b[1;32m    182\u001b[0m                 \u001b[0;34m\"Length mismatch: Expected axis has {old} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m                 \u001b[0;34m\"values have {new} elements\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m             )\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 4 elements, new values have 3 elements"
     ]
    }
   ],
   "source": [
    "# 加载邀请回答数据\n",
    "\n",
    "train = pd.read_csv(f'{base_path}/invite_info_0926.txt', sep='\\t', header=None)\n",
    "train.columns = ['qid', 'uid', 'dt', 'label']\n",
    "logging.info(\"invite %s\", train.shape)\n",
    "\n",
    "# test = pd.read_csv(f'{base_path}/invite_info_evaluate_1_0926.txt', sep='\\t', header=None)\n",
    "test = pd.read_csv(f'{base_path}/sample_sub_1.txt', sep='\\t', header=None)\n",
    "test.columns = ['qid', 'uid', 'dt','pre']\n",
    "logging.info(\"test %s\", test.shape)\n",
    "\n",
    "sub = test.copy()\n",
    "\n",
    "sub_size = len(sub)\n",
    "\n",
    "train['day'] = extract_day(train['dt'])\n",
    "train['hour'] = extract_hour(train['dt'])\n",
    "\n",
    "test['day'] = extract_day(test['dt'])\n",
    "test['hour'] = extract_hour(test['dt'])\n",
    "del train['dt'], test['dt'],test['pre']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载问题\n",
    "ques = pd.read_csv(f'{base_path}/question_info_0926.txt', header=None, sep='\\t')\n",
    "ques.columns = ['qid', 'q_dt', 'title_t1', 'title_t2', 'desc_t1', 'desc_t2', 'q_topic']\n",
    "# del ques['title_t1'], ques['title_t2'], ques['desc_t1'], ques['desc_t2']\n",
    "del ques['title_t1'],ques['desc_t1']\n",
    "logging.info(\"ques %s\", ques.shape)\n",
    "\n",
    "ques['q_day'] = extract_day(ques['q_dt'])\n",
    "ques['q_hour'] = extract_hour(ques['q_dt'])\n",
    "del ques['q_dt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques['关注话题'] = train['关注话题'].apply(parse_list_1)\n",
    "ques['title_t2'] = ques['title_t2'].apply(parse_list_1)\n",
    "ques['desc_t2'] = ques['desc_t2'].apply(parse_list_1)\n",
    "ques['q_topic'] = ques['q_topic'].apply(parse_list_1)\n",
    "# train['感兴趣话题'] = train['感兴趣话题'].apply(parse_map)\n",
    "ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmap = pd.read_csv('../data/topic_vectors_64d.txt', \n",
    "                          names=['id', 'embed'], sep='\\t')\n",
    "topicmap['embed'] = topicmap['embed'].apply(parse_str)\n",
    "topicmap['id'] = topicmap['id'].apply(lambda x: int(x[1:]))\n",
    "topicmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topicmap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vector_dict = dict(zip(np.array(topicmap['id']), np.array(topicmap['embed'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic2v(x):\n",
    "    try:\n",
    "        tmp = topic_vector_dict[x[0]]\n",
    "    except:\n",
    "        tmp = np.zeros(64)\n",
    "    for i in x[1:]:\n",
    "        tmp = tmp + topic_vector_dict[i]\n",
    "    if len(tmp) == 0:\n",
    "        return np.zeros(64)\n",
    "    return (tmp / len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"topic2v...\")\n",
    "ques['q_topic']=ques['q_topic'].progress_apply(lambda x:topic2v(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmap = pd.read_csv('../data/word_vectors_64d.txt', \n",
    "                          names=['id', 'embed'], sep='\\t')\n",
    "wordmap['embed'] = wordmap['embed'].apply(parse_str)\n",
    "wordmap['id'] = wordmap['id'].apply(lambda x: int(x[1:]))\n",
    "wordmap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordmap.shape\n",
    "word_vector_dict = dict(zip(np.array(wordmap['id']), np.array(wordmap['embed'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2v(x):\n",
    "    try:\n",
    "        tmp = word_vector_dict[x[0]]\n",
    "    except:\n",
    "        tmp = np.zeros(64)\n",
    "    for i in x[1:]:\n",
    "        tmp = tmp + word_vector_dict[i]\n",
    "    if len(tmp) == 0:\n",
    "        return np.zeros(64)\n",
    "    return (tmp / len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"word2v...\")\n",
    "ques['title_t2']=ques['title_t2'].progress_apply(lambda x:word2v(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"word2v...\")\n",
    "ques['desc_t2']=ques['desc_t2'].progress_apply(lambda x:word2v(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordadd(x):\n",
    "    try:\n",
    "        tmp = 0.0\n",
    "    except:\n",
    "        tmp = 0.0\n",
    "    tmp=x.sum()\n",
    "    return (tmp /64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"wordadd...\")\n",
    "ques['title_t2']=ques['title_t2'].progress_apply(lambda x:wordadd(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"wordadd...\")\n",
    "ques['desc_t2']=ques['desc_t2'].progress_apply(lambda x:wordadd(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"wordadd...\")\n",
    "ques['q_topic']=ques['q_topic'].progress_apply(lambda x:wordadd(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载回答\n",
    "ans = pd.read_csv(f'{base_path}/answer_info_0926.txt', header=None, sep='\\t')\n",
    "ans.columns = ['aid', 'qid', 'uid', 'ans_dt', 'ans_t1', 'ans_t2', 'is_good', 'is_rec', 'is_dest', 'has_img',\n",
    "               'has_video', 'word_count', 'reci_cheer', 'reci_uncheer', 'reci_comment', 'reci_mark', 'reci_tks',\n",
    "               'reci_xxx', 'reci_no_help', 'reci_dis']\n",
    "del ans['ans_t1'], ans['ans_t2']\n",
    "logging.info(\"ans %s\", ans.shape)\n",
    "\n",
    "ans['a_day'] = extract_day(ans['ans_dt'])\n",
    "ans['a_hour'] = extract_hour(ans['ans_dt'])\n",
    "del ans['ans_dt']\n",
    "\n",
    "ans = pd.merge(ans, ques, on='qid')\n",
    "del ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ans['a_day'].min())\n",
    "print(ans['a_day'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 回答距提问的天数\n",
    "ans['diff_qa_days'] = ans['a_day'] - ans['q_day']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间窗口划分\n",
    "# train\n",
    "# val\n",
    "train_start = 3838\n",
    "train_end = 3867\n",
    "\n",
    "val_start = 3868\n",
    "val_end = 3874\n",
    "\n",
    "label_end = 3867\n",
    "label_start = label_end - 6\n",
    "\n",
    "train_label_feature_end = label_end - 7\n",
    "train_label_feature_start = train_label_feature_end - 22\n",
    "\n",
    "train_ans_feature_end = label_end - 7\n",
    "train_ans_feature_start = train_ans_feature_end - 50\n",
    "\n",
    "val_label_feature_end = val_start - 1\n",
    "val_label_feature_start = val_label_feature_end - 22\n",
    "\n",
    "val_ans_feature_end = val_start - 1\n",
    "val_ans_feature_start = val_ans_feature_end - 50\n",
    "\n",
    "train_label_feature = train[(train['day'] >= train_label_feature_start) & (train['day'] <= train_label_feature_end)]\n",
    "logging.info(\"train_label_feature %s\", train_label_feature.shape)\n",
    "\n",
    "val_label_feature = train[(train['day'] >= val_label_feature_start) & (train['day'] <= val_label_feature_end)]\n",
    "logging.info(\"val_label_feature %s\", val_label_feature.shape)\n",
    "\n",
    "train_label = train[(train['day'] > train_label_feature_end)]\n",
    "\n",
    "logging.info(\"train feature start %s end %s, label start %s end %s\", train_label_feature['day'].min(),\n",
    "             train_label_feature['day'].max(), train_label['day'].min(), train_label['day'].max())\n",
    "\n",
    "logging.info(\"test feature start %s end %s, label start %s end %s\", val_label_feature['day'].min(),\n",
    "             val_label_feature['day'].max(), test['day'].min(), test['day'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定ans的时间范围\n",
    "# 3807~3874\n",
    "train_ans_feature = ans[(ans['a_day'] >= train_ans_feature_start) & (ans['a_day'] <= train_ans_feature_end)]\n",
    "\n",
    "val_ans_feature = ans[(ans['a_day'] >= val_ans_feature_start) & (ans['a_day'] <= val_ans_feature_end)]\n",
    "\n",
    "logging.info(\"train ans feature %s, start %s end %s\", train_ans_feature.shape, train_ans_feature['a_day'].min(),\n",
    "             train_ans_feature['a_day'].max())\n",
    "\n",
    "logging.info(\"val ans feature %s, start %s end %s\", val_ans_feature.shape, val_ans_feature['a_day'].min(),\n",
    "             val_ans_feature['a_day'].max())\n",
    "\n",
    "fea_cols = ['is_good', 'is_rec', 'is_dest', 'has_img', 'has_video', 'word_count',\n",
    "            'reci_cheer', 'reci_uncheer', 'reci_comment', 'reci_mark', 'reci_tks',\n",
    "            'reci_xxx', 'reci_no_help', 'reci_dis', 'diff_qa_days']\n",
    "\n",
    "\n",
    "def extract_feature1(target, label_feature, ans_feature):\n",
    "    # 问题特征\n",
    "    t1 = label_feature.groupby('qid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "    t1.columns = ['qid', 'q_inv_mean', 'q_inv_sum', 'q_inv_std', 'q_inv_count']\n",
    "    target = pd.merge(target, t1, on='qid', how='left')\n",
    "\n",
    "    # 用户特征\n",
    "    t1 = label_feature.groupby('uid')['label'].agg(['mean', 'sum', 'std', 'count']).reset_index()\n",
    "    t1.columns = ['uid', 'u_inv_mean', 'u_inv_sum', 'u_inv_std', 'u_inv_count']\n",
    "    target = pd.merge(target, t1, on='uid', how='left')\n",
    "    #\n",
    "    # train_size = len(train)\n",
    "    # data = pd.concat((train, test), sort=True)\n",
    "\n",
    "    # 回答部分特征\n",
    "\n",
    "    t1 = ans_feature.groupby('qid')['aid'].count().reset_index()\n",
    "    t1.columns = ['qid', 'q_ans_count']\n",
    "    target = pd.merge(target, t1, on='qid', how='left')\n",
    "\n",
    "    t1 = ans_feature.groupby('uid')['aid'].count().reset_index()\n",
    "    t1.columns = ['uid', 'u_ans_count']\n",
    "    target = pd.merge(target, t1, on='uid', how='left')\n",
    "\n",
    "#     for col in fea_cols:\n",
    "#         t1 = ans_feature.groupby('uid')[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "#         t1.columns = ['uid', f'u_{col}_sum', f'u_{col}_max', f'u_{col}_mean']\n",
    "#         target = pd.merge(target, t1, on='uid', how='left')\n",
    "\n",
    "#         t1 = ans_feature.groupby('qid')[col].agg(['sum', 'max', 'mean']).reset_index()\n",
    "#         t1.columns = ['qid', f'q_{col}_sum', f'q_{col}_max', f'q_{col}_mean']\n",
    "#         target = pd.merge(target, t1, on='qid', how='left')\n",
    "#         logging.info(\"extract %s\", col)\n",
    "    return target\n",
    "\n",
    "\n",
    "train_label = extract_feature1(train_label, train_label_feature, train_ans_feature)\n",
    "test = extract_feature1(test, val_label_feature, val_ans_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征提取结束\n",
    "logging.info(\"train shape %s, test shape %s\", train_label.shape, test.shape)\n",
    "assert len(test) == sub_size\n",
    "\n",
    "# 加载用户\n",
    "user = pd.read_csv(f'{base_path}/member_info_0926.txt', header=None, sep='\\t')\n",
    "user.columns = ['uid', 'gender', 'creat_keyword', 'level', 'hot', 'reg_type', 'reg_plat', 'freq', 'uf_b1', 'uf_b2',\n",
    "                'uf_b3', 'uf_b4', 'uf_b5', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5', 'score', 'follow_topic',\n",
    "                'inter_topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dit = {'daily': 4, 'weekly': 3, 'monthly': 2, 'new': 1,'unknow':0}\n",
    "user['freq'] = user['freq'].map(dit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = user.drop(['creat_keyword','level','hot','reg_type','reg_plat'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user['follow_topic'] = user['follow_topic'].apply(parse_list_1)\n",
    "user['inter_topic'] = user['inter_topic'].apply(parse_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_interest2v(x):\n",
    "    if len(x)==0:\n",
    "        return np.zeros(64)\n",
    "    else:\n",
    "        tmp=np.zeros(64)\n",
    "        for i in x:\n",
    "            tmp = tmp + topic_vector_dict[i]*x[i]\n",
    "        return (tmp / len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"topic2v...\")\n",
    "user['follow_topic']=user['follow_topic'].progress_apply(lambda x:topic2v(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"topic_interest2v...\")\n",
    "user['inter_topic']=user['inter_topic'].progress_apply(lambda x:topic_interest2v(x))\n",
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"wordadd...\")\n",
    "user['follow_topic']=user['follow_topic'].progress_apply(lambda x:wordadd(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"wordadd...\")\n",
    "user['inter_topic']=user['inter_topic'].progress_apply(lambda x:wordadd(x))\n",
    "print('finished!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info(\"user %s\", user.shape)\n",
    "\n",
    "unq = user.nunique()\n",
    "logging.info(\"user unq %s\", unq)\n",
    "\n",
    "for x in unq[unq == 1].index:\n",
    "    del user[x]\n",
    "    logging.info('del unq==1 %s', x)\n",
    "\n",
    "t = user.dtypes\n",
    "cats = [x for x in t[t == 'object'].index if x not in ['follow_topic', 'inter_topic', 'uid','freq']]\n",
    "logging.info(\"user cat %s\", cats)\n",
    "\n",
    "for d in cats:\n",
    "    lb = LabelEncoder()\n",
    "    user[d] = lb.fit_transform(user[d])\n",
    "    logging.info('encode %s', d)\n",
    "\n",
    "q_lb = LabelEncoder()\n",
    "q_lb.fit(list(train_label['qid'].astype(str).values) + list(test['qid'].astype(str).values))\n",
    "train_label['qid_enc'] = q_lb.transform(train_label['qid'])\n",
    "test['qid_enc'] = q_lb.transform(test['qid'])\n",
    "\n",
    "u_lb = LabelEncoder()\n",
    "u_lb.fit(user['uid'])\n",
    "train_label['uid_enc'] = u_lb.transform(train_label['uid'])\n",
    "test['uid_enc'] = u_lb.transform(test['uid'])\n",
    "\n",
    "# merge user\n",
    "train_label = pd.merge(train_label, user, on='uid', how='left')\n",
    "test = pd.merge(test, user, on='uid', how='left')\n",
    "logging.info(\"train shape %s, test shape %s\", train_label.shape, test.shape)\n",
    "\n",
    "data = pd.concat((train_label, test), axis=0, sort=True)\n",
    "# del train_label, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count编码\n",
    "count_fea = ['uid_enc', 'qid_enc', 'gender', 'freq', 'uf_c1', 'uf_c2', 'uf_c3', 'uf_c4', 'uf_c5']\n",
    "for feat in count_fea:\n",
    "    col_name = '{}_count'.format(feat)\n",
    "    data[col_name] = data[feat].map(data[feat].value_counts().astype(int))\n",
    "    data.loc[data[col_name] < 2, feat] = -1\n",
    "    data[feat] += 1\n",
    "    data[col_name] = data[feat].map(data[feat].value_counts().astype(int))\n",
    "    data[col_name] = (data[col_name] - data[col_name].min()) / (data[col_name].max() - data[col_name].min())\n",
    "#     # \n",
    "\n",
    "# # 问题被回答的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 压缩数据\n",
    "t = data.dtypes\n",
    "for x in t[t == 'int64'].index:\n",
    "    data[x] = data[x].astype('int32')\n",
    "\n",
    "for x in t[t == 'float64'].index:\n",
    "    data[x] = data[x].astype('float32')\n",
    "\n",
    "data['wk'] = data['day'] % 7\n",
    "\n",
    "feature_cols = [x for x in data.columns if x not in ('label', 'uid', 'qid', 'dt', 'day')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target编码\n",
    "logging.info(\"feature size %s\", len(feature_cols))\n",
    "\n",
    "X_train_all = data.iloc[:len(train_label)][feature_cols]\n",
    "y_train_all = data.iloc[:len(train_label)]['label']\n",
    "test = data.iloc[len(train_label):]\n",
    "# del data\n",
    "assert len(test) == sub_size\n",
    "\n",
    "logging.info(\"train shape %s, test shape %s\", train_label.shape, test.shape)\n",
    "\n",
    "fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for index, (train_idx, val_idx) in enumerate(fold.split(X=X_train_all, y=y_train_all)):\n",
    "    break\n",
    "\n",
    "X_train, X_val, y_train, y_val = X_train_all.iloc[train_idx][feature_cols], X_train_all.iloc[val_idx][feature_cols], \\\n",
    "                                 y_train_all.iloc[train_idx], \\\n",
    "                                 y_train_all.iloc[val_idx]\n",
    "del X_train_all\n",
    "\n",
    "model_lgb = LGBMClassifier(n_estimators=6000, n_jobs=-1, objective='binary', seed=1000, silent=True)\n",
    "model_lgb.fit(X_train, y_train,\n",
    "              eval_metric=['logloss', 'auc'],\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              early_stopping_rounds=100)\n",
    "\n",
    "sub['label'] = model_lgb.predict_proba(test[feature_cols])[:, 1]\n",
    "\n",
    "\n",
    "sub.to_csv('../example/result_1217_1.txt', index=None, header=None, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
